```python
import snowflake.connector
import pandas as pd
from dotenv import load_dotenv
import os
import json
from google.cloud import storage, bigquery
from google.oauth2 import service_account
from io import StringIO
import pyarrow as pa
import pyarrow.parquet as pq
from datetime import datetime


# โหลดค่าจากไฟล์ .env
load_dotenv()

# สร้างการเชื่อมต่อกับ Snowflake
conn = snowflake.connector.connect(
    user= os.getenv("user"),
    password= os.getenv("password"),
    account= os.getenv("account"),
    warehouse= os.getenv("warehouse"),
    database= os.getenv("database"),
    schema= os.getenv("schema")
)

snowflake_schema = 'TPCDS_SF10TCL'



# กำหนด path ของไฟล์ Service Account Key
service_account_path = os.getenv("service_account_path")

# สร้าง Credentials จากไฟล์ Service Account
credentials = service_account.Credentials.from_service_account_file(service_account_path)

# สร้าง client สำหรับเชื่อมต่อกับ BigQuery โดยใช้ credentials ที่กำหนดเอง
client_bq = bigquery.Client(credentials=credentials, project=credentials.project_id)
client_gcs = storage.Client(credentials=credentials, project=credentials.project_id)


# Google Cloud Storage and BigQuery settings
gcs_bucket_name = 'ojsnd-data-landing-zone'
gcs_folder = 'ojsnd-data-landing-zone/snowflake'  
project_id = credentials.project_id
dataset_id = 'wh_staging'


# Read the JSON configuration file
def load_json_config(json_file):
    with open(json_file, 'r') as file:
        config = json.load(file)
    return config

# Function to get the max timestamp from the BigQuery destination table
def get_max_timestamp_from_bq(table_name, timestamp_columns):
    # Generate the `MAX()` condition for each timestamp column
    timestamp_conditions = ', '.join([f"MAX({col}) AS max_{col}" for col in timestamp_columns])
    
    # Construct the query to get max timestamps from BigQuery
    query = f"""
        SELECT {timestamp_conditions}
        FROM `{project_id}.{dataset_id}.{table_name}`
    """
    
    # Run the query
    print(query)
    query_job = client_bq.query(query)
    result = query_job.result()
    
    max_timestamps = {}
    for row in result:
        for col in timestamp_columns:
            max_timestamps[col] = row[f"max_{col}"]
    
    return max_timestamps
# Function to export new data from Snowflake
def export_new_data_from_snowflake(table_name, timestamp_columns, max_timestamp):
    # Construct the query to export data from Snowflake, using the max timestamp for each timestamp column
    timestamp_conditions = ' AND '.join([
        f"{col} > '{max_timestamp.get(col)}'" if "date" in col.lower() else f"{col} > {max_timestamp.get(col)}"
        for col in timestamp_columns
    ])
    query = f"""
        SELECT *
        FROM {snowflake_schema}.{table_name}
        WHERE {timestamp_conditions}
        LIMIT 100
    """
    print(query)
    cursor = conn.cursor()
    cursor.execute(query)

    # Fetch the data into a pandas DataFrame
    df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])

    # Convert the DataFrame to a Parquet file using PyArrow
    export_date = datetime.now().strftime("%Y-%m-%d")
    file_name = f"{table_name}_{export_date}.parquet"
    file_path = f"{gcs_folder}/{file_name}"
    
    table = pa.Table.from_pandas(df)
    pq.write_table(table, '/tmp/' + file_name)

    # Upload the Parquet file to Google Cloud Storage
    bucket = client_gcs.get_bucket(gcs_bucket_name)
    blob = bucket.blob(file_path)
    blob.upload_from_filename('/tmp/' + file_name)

    print(f"Exported new data to GCS: {file_path}")
    cursor.close()

    return file_path

# Function to load the exported Parquet file into BigQuery
def load_parquet_to_bq(file_path, table_name):
    # Define the BigQuery table reference
    table_ref = client_bq.dataset(dataset_id).table(table_name)

    # Define the URI for the Parquet file in GCS
    gcs_uri = f"gs://{gcs_bucket_name}/{file_path}"

    # Configure the load job
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.PARQUET,
        autodetect=True,  # Automatically detect schema from the Parquet file
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE  # Replace table (truncate)
    )

    # Load the Parquet data from GCS into BigQuery
    load_job = client_bq.load_table_from_uri(
        gcs_uri, table_ref, job_config=job_config
    )

    load_job.result()  # Wait for the job to complete
    print(f"Loaded new data into BigQuery table: {table_name}")

# Main workflow
def main(json_file):
    # Load JSON configuration
    config = load_json_config(json_file)

    for table_info in config:
        table_name = table_info['table_name']
        timestamp_columns = table_info['timestamp_columns']  # List of timestamp columns
        max_timestamp = get_max_timestamp_from_bq(table_name, timestamp_columns)

        # If there's no data in BigQuery, set max_timestamp to a very early date
        if not max_timestamp:
            max_timestamp = '1970-01-01 00:00:00'

        print(f"Max timestamp for {table_name}: {max_timestamp}")

        # Step 2: Export new data from Snowflake based on the max timestamp
        file_path = export_new_data_from_snowflake(table_name, timestamp_columns, max_timestamp)

        # Step 3: Load the exported Parquet file into BigQuery
        load_parquet_to_bq(file_path, table_name)

    # Close the Snowflake connection
    conn.close()

# Run the main function with the JSON config file
main('tables_config.json')
